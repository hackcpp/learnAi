{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-deepseek\n",
    "%pip install langchain-ollama\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\",override=True)\n",
    "from os import getenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "key = getenv(\"DEEPSEEK_API_KEY\") \n",
    "url = getenv(\"DEEPSEEK_API_BASE\")\n",
    "model = getenv(\"DEEPSEEK_MODEL\")\n",
    "\n",
    "# print(key, url, model)\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "key = getenv(\"SF_API_KEY\") \n",
    "url = getenv(\"SF_API_BASE\")\n",
    "model = getenv(\"SF_MODEL\")\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = key\n",
    "os.environ[\"DEEPSEEK_API_BASE\"] = url\n",
    "print(key, url, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to chinese. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Chinese.\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), \n",
    "     (\"user\", human_template)]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "# print(chain)\n",
    "print(chain.invoke({\"language\": \"Chinese\", \"text\": \"hi!\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template( \"Translate the following from English into {language}\")\n",
    "human_template = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_template, \n",
    "     human_template]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "# print(chain)\n",
    "print(chain.invoke({\"language\": \"Chinese\", \"text\": \"hi!\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "input_data = \"What is the capital of France?\"\n",
    "summary_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Summarize this: {input}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "sentiment_chain = (\n",
    "    ChatPromptTemplate.from_template(\"What is the sentiment of this: {input}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"sentiment\": sentiment_chain\n",
    "})\n",
    "parallel_result = parallel_chain.invoke({\"input\": input_data})\n",
    "print(parallel_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 定义子链\n",
    "tech_prompt = ChatPromptTemplate.from_template(\n",
    "    \"作为科技作者，用专业术语解释：{concept}\"\n",
    ")\n",
    "sports_prompt = ChatPromptTemplate.from_template(\n",
    "    \"作为体育解说员，用生动语言描述：{concept}\"\n",
    ")\n",
    "general_prompt = ChatPromptTemplate.from_template(\n",
    "    \"用通俗语言解释：{concept}\"\n",
    ")\n",
    "\n",
    "tech_chain = tech_prompt | llm | StrOutputParser()\n",
    "sports_chain = sports_prompt | llm | StrOutputParser()\n",
    "general_chain = general_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 构建分支逻辑\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: x[\"topic\"] == \"科技\", tech_chain),\n",
    "    (lambda x: x[\"topic\"] == \"体育\", sports_chain),\n",
    "    general_chain\n",
    ")\n",
    "\n",
    "# 完整执行链\n",
    "# full_chain = RunnableParallel({  # 并行处理输入\n",
    "#     \"concept\": lambda x: x[\"concept\"],\n",
    "#     \"topic\": lambda x: x[\"topic\"]\n",
    "# }) | branch_chain\n",
    "full_chain = branch_chain\n",
    "\n",
    "# 测试不同分支\n",
    "print(full_chain.invoke({\n",
    "    \"concept\": \"神经网络\", \n",
    "    \"topic\": \"科技\"\n",
    "}))\n",
    "# 输出包含\"神经元\"、\"反向传播\"等术语\n",
    "\n",
    "print(full_chain.invoke({\n",
    "    \"concept\": \"越位\", \n",
    "    \"topic\": \"体育\"\n",
    "}))\n",
    "# 输出包含\"足球比赛\"、\"裁判判罚\"等描述\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
