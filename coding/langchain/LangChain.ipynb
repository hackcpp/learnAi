{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-deepseek\n",
    "%pip install langchain-ollama\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\",override=True)\n",
    "from os import getenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "key = getenv(\"DEEPSEEK_API_KEY\") \n",
    "url = getenv(\"DEEPSEEK_API_BASE\")\n",
    "model = getenv(\"DEEPSEEK_MODEL\")\n",
    "\n",
    "# print(key, url, model)\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "key = getenv(\"SF_API_KEY\") \n",
    "url = getenv(\"SF_API_BASE\")\n",
    "model = getenv(\"SF_MODEL\")\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = key\n",
    "os.environ[\"DEEPSEEK_API_BASE\"] = url\n",
    "print(key, url, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to chinese. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Chinese.\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), \n",
    "     (\"user\", human_template)]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "# print(chain)\n",
    "print(chain.invoke({\"language\": \"Chinese\", \"text\": \"hi!\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template( \"Translate the following from English into {language}\")\n",
    "human_template = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_template, \n",
    "     human_template]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "# print(chain)\n",
    "print(chain.invoke({\"language\": \"Chinese\", \"text\": \"hi!\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "input_data = \"What is the capital of France?\"\n",
    "summary_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Summarize this: {input}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "sentiment_chain = (\n",
    "    ChatPromptTemplate.from_template(\"What is the sentiment of this: {input}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"sentiment\": sentiment_chain\n",
    "})\n",
    "parallel_result = parallel_chain.invoke({\"input\": input_data})\n",
    "print(parallel_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各位观众朋友们，让我们来聊聊足球场上那个让人又爱又恨的规则——越位！\n",
      "\n",
      "想象一下，比赛正进行到白热化阶段，前锋如同一只猎豹，突然加速，甩开防守球员，接到队友的精准传球，一脚劲射破门！全场沸腾，球迷们欢呼雀跃！然而，就在这激动人心的时刻，边裁举起了手中的旗帜，主裁判也吹响了哨声——**越位！进球无效！**\n",
      "\n",
      "越位，这个规则就像足球场上的“隐形裁判”，时刻盯着球员们的一举一动。简单来说，当进攻球员在接球的瞬间，**身体的有效部位（通常是脚或躯干）**比倒数第二名防守球员（通常是最后一名后卫）更靠近对方球门，并且处于对方半场，那么他就越位了。除非，他接球时在自己的半场，或者他接球时与倒数第二名防守球员平行。\n",
      "\n",
      "越位的判罚往往伴随着争议，因为它需要裁判在电光火石之间做出判断。有时，球员只是脚尖越过了那条“隐形线”，进球就被无情地取消；有时，裁判的视线被遮挡，误判也会发生。但正是这种精确到毫厘的规则，让足球比赛充满了悬念和戏剧性。\n",
      "\n",
      "越位就像一场“猫鼠游戏”，前锋们总是试图在最后一刻“偷跑”，而防守球员则拼命保持阵型，不让对手有机可乘。每一次越位判罚，都可能改变比赛的走向，甚至决定胜负。\n",
      "\n",
      "所以，下次当你看到边裁举旗，听到裁判哨声，别急着失望或欢呼，先看看是不是越位！这就是足球，规则与激情并存，细节决定成败！\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 定义子链\n",
    "tech_prompt = ChatPromptTemplate.from_template(\n",
    "    \"作为科技作者，用专业术语解释：{concept}\"\n",
    ")\n",
    "sports_prompt = ChatPromptTemplate.from_template(\n",
    "    \"作为体育解说员，用生动语言描述：{concept}\"\n",
    ")\n",
    "general_prompt = ChatPromptTemplate.from_template(\n",
    "    \"用通俗语言解释：{concept}\"\n",
    ")\n",
    "\n",
    "tech_chain = tech_prompt | llm | StrOutputParser()\n",
    "sports_chain = sports_prompt | llm | StrOutputParser()\n",
    "general_chain = general_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 构建分支逻辑\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: x[\"topic\"] == \"科技\", tech_chain),\n",
    "    (lambda x: x[\"topic\"] == \"体育\", sports_chain),\n",
    "    general_chain\n",
    ")\n",
    "\n",
    "# 完整执行链\n",
    "# full_chain = RunnableParallel({  # 并行处理输入\n",
    "#     \"concept\": lambda x: x[\"concept\"],\n",
    "#     \"topic\": lambda x: x[\"topic\"]\n",
    "# }) | branch_chain\n",
    "full_chain = branch_chain\n",
    "\n",
    "print(\"***********************\")\n",
    "print(full_chain.invoke({\n",
    "    \"concept\": \"神经网络\", \n",
    "    \"topic\": \"科技\"\n",
    "}))\n",
    "print(\"***********************\")\n",
    "\n",
    "print(full_chain.invoke({\n",
    "    \"concept\": \"越位\", \n",
    "    \"topic\": \"体育\"\n",
    "}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
