{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-deepseek\n",
    "%pip install langchain-ollama\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 自动从 .env 加载变量到系统环境\n",
    "load_dotenv(override=True)  \n",
    "\n",
    "os.environ\n",
    "\n",
    "# 读取变量\n",
    "# DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "# print(DEEPSEEK_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = os.environ[\"SF_API_KEY\"]\n",
    "os.environ[\"DEEPSEEK_API_BASE\"] = os.environ[\"SF_API_BASE\"]\n",
    "\n",
    "# print(os.environ[\"SF_API_BASE\"])\n",
    "# print(os.environ[\"SF_API_KEY\"])\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to chinese. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = os.environ[\"SF_API_KEY\"]\n",
    "os.environ[\"DEEPSEEK_API_BASE\"] = os.environ[\"SF_API_BASE\"]\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "input_data = \"What is the capital of France?\"\n",
    "summary_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Summarize this: {input}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "sentiment_chain = (\n",
    "    ChatPromptTemplate.from_template(\"What is the sentiment of this: {input}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"sentiment\": sentiment_chain\n",
    "})\n",
    "parallel_result = parallel_chain.invoke({\"input\": input_data})\n",
    "print(parallel_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = os.environ[\"SF_API_KEY\"]\n",
    "os.environ[\"DEEPSEEK_API_BASE\"] = os.environ[\"SF_API_BASE\"]\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "input_data = \"What is the capital of France?\"\n",
    "summary_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Summarize this: {input}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "sentiment_chain = (\n",
    "    ChatPromptTemplate.from_template(\"What is the sentiment of this: {input}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"sentiment\": sentiment_chain\n",
    "})\n",
    "parallel_result = parallel_chain.invoke({\"input\": input_data})\n",
    "print(parallel_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Chinese.\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "# print(chain)\n",
    "print(chain.invoke({\"language\": \"Chinese\", \"text\": \"hi!\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个{role}\"),\n",
    "    (\"human\", \"根据用户描述生成{format}：{input}\")\n",
    "])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\n",
    "    \"role\": \"诗人\", \n",
    "    \"format\": \"五言绝句\",\n",
    "    \"input\": \"秋天的夜晚\"\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/chain/\")\n",
    "remote_chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = os.environ[\"SF_API_KEY\"]\n",
    "os.environ[\"DEEPSEEK_API_BASE\"] = os.environ[\"SF_API_BASE\"]\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "messages = [HumanMessage(\"what is 11 add 49?\")]\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "# print(ai_msg)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "tool_calls = ai_msg.tool_calls\n",
    "# print(tool_calls)\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    # print(tool_msg)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "# print(messages)\n",
    "ai_msg = llm_with_tools.invoke(messages) \n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = os.environ[\"SF_API_KEY\"]\n",
    "os.environ[\"DEEPSEEK_API_BASE\"] = os.environ[\"SF_API_BASE\"]\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 定义子链\n",
    "tech_prompt = ChatPromptTemplate.from_template(\n",
    "    \"作为科技作者，用专业术语解释：{concept}\"\n",
    ")\n",
    "sports_prompt = ChatPromptTemplate.from_template(\n",
    "    \"作为体育解说员，用生动语言描述：{concept}\"\n",
    ")\n",
    "general_prompt = ChatPromptTemplate.from_template(\n",
    "    \"用通俗语言解释：{concept}\"\n",
    ")\n",
    "\n",
    "tech_chain = tech_prompt | llm | StrOutputParser()\n",
    "sports_chain = sports_prompt | llm | StrOutputParser()\n",
    "general_chain = general_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 构建分支逻辑\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: x[\"topic\"] == \"科技\", tech_chain),\n",
    "    (lambda x: x[\"topic\"] == \"体育\", sports_chain),\n",
    "    general_chain\n",
    ")\n",
    "\n",
    "# 完整执行链\n",
    "# full_chain = RunnableParallel({  # 并行处理输入\n",
    "#     \"concept\": lambda x: x[\"concept\"],\n",
    "#     \"topic\": lambda x: x[\"topic\"]\n",
    "# }) | branch_chain\n",
    "full_chain = branch_chain\n",
    "\n",
    "# 测试不同分支\n",
    "print(full_chain.invoke({\n",
    "    \"concept\": \"神经网络\", \n",
    "    \"topic\": \"科技\"\n",
    "}))\n",
    "# 输出包含\"神经元\"、\"反向传播\"等术语\n",
    "\n",
    "print(full_chain.invoke({\n",
    "    \"concept\": \"越位\", \n",
    "    \"topic\": \"体育\"\n",
    "}))\n",
    "# 输出包含\"足球比赛\"、\"裁判判罚\"等描述\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提示内容:\n",
      " [HumanMessage(content='总结文本：这款手机拍照效果出色，但电池续航较差', additional_kwargs={}, response_metadata={})]\n",
      "{'summary': '## 手机优缺点总结：\\n\\n**优点：**\\n\\n* 拍照效果出色\\n\\n**缺点：**\\n\\n* 电池续航较差', 'sentiment': '这段文本的情感倾向是**中性偏负面**。具体分析如下：\\n\\n1. **正面情感**：提到“拍照效果出色”，这是对手机的一个积极评价，表明用户对手机的拍照功能非常满意。\\n2. **负面情感**：提到“电池续航较差”，这是对手机的一个消极评价，表明用户对电池续航能力不满意。\\n\\n虽然文本中同时包含了正面和负面的评价，但由于电池续航是用户日常使用中非常重要的一个方面，因此整体情感倾向偏向负面。'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "\n",
    "# 导入环境变量\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = os.environ[\"SF_API_KEY\"]\n",
    "os.environ[\"DEEPSEEK_API_BASE\"] = os.environ[\"SF_API_BASE\"]\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 定义子链\n",
    "prompt = ChatPromptTemplate.from_template(\"总结文本：{text}\")\n",
    "# debug_prompt = prompt | (lambda x: print(\"提示内容:\\n\", x.to_messages()) or x)\n",
    "summary_chain = (\n",
    "    prompt | (lambda x: print(\"提示内容:\\n\", x.to_messages()) or x)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "sentiment_chain = (\n",
    "    ChatPromptTemplate.from_template(\"分析文本情感倾向：{text}\")\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 并行处理链\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"sentiment\": sentiment_chain\n",
    "})\n",
    "\n",
    "# 执行\n",
    "input_data = {\"text\": \"这款手机拍照效果出色，但电池续航较差\"}\n",
    "result = parallel_chain.invoke(input_data)\n",
    "print(result)\n",
    "\n",
    "#print(f\"总结：{result['summary']}\")\n",
    "# 输出：总结：该手机拍照功能优秀，但电池续航能力不足\n",
    "#print(f\"情感：{result['sentiment']}\")\n",
    "# 输出：情感：正面与负面评价并存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "# 导入环境变量\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = os.environ[\"SF_API_KEY\"]\n",
    "os.environ[\"DEEPSEEK_API_BASE\"] = os.environ[\"SF_API_BASE\"]\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 定义子链\n",
    "prompt = ChatPromptTemplate.from_template(\"总结文本：{text}\")\n",
    "# debug_prompt = prompt | (lambda x: print(\"提示内容:\\n\", x.to_messages()) or x)\n",
    "summary_chain = (\n",
    "    prompt | (lambda x: print(\"提示内容:\\n\", x.to_messages()) or x)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "sentiment_chain = (\n",
    "    ChatPromptTemplate.from_template(\"分析文本情感倾向：{text}\")\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 并行处理链\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"sentiment\": sentiment_chain\n",
    "})\n",
    "\n",
    "# 执行\n",
    "input_data = {\"text\": \"这款手机拍照效果出色，但电池续航较差\"}\n",
    "result = parallel_chain.invoke(input_data)\n",
    "# print(result)\n",
    "\n",
    "#print(f\"总结：{result['summary']}\")\n",
    "# 输出：总结：该手机拍照功能优秀，但电池续航能力不足\n",
    "#print(f\"情感：{result['sentiment']}\")\n",
    "# 输出：情感：正面与负面评价并存\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
